# -*- coding: utf-8 -*-
"""ResearchModule1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ziDO-BUEFHgpUAwpYxE4cJxznsZZK76o
"""

# Install required packages
!pip install torch torchvision opencv-python matplotlib scikit-learn tqdm pillow

# Import all necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from google.colab import drive
from torch.cuda.amp import autocast, GradScaler
import gc
import warnings
warnings.filterwarnings('ignore')

# Clear GPU memory from previous runs
torch.cuda.empty_cache()
gc.collect()

# Mount Google Drive
print("Mounting Google Drive...")
drive.mount('/content/drive')

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Memory-Optimized Configuration
DATA_DIR = '/content/drive/MyDrive/KMU_FED_DATASET'
BATCH_SIZE = 16  # Reduced from 64 to prevent OOM
ACCUM_STEPS = 4  # Gradient accumulation to maintain effective batch size of 64
SEQUENCE_LENGTH = 4
NUM_EPOCHS = 100
LEARNING_RATE = 0.001
WEIGHT_DECAY = 1e-4
PATIENCE = 15
USE_AMP = True  # Mixed precision for memory efficiency

# Progressive training phases
PHASE_1_EMOTIONS = ['happy', 'sad']
PHASE_2_EMOTIONS = ['happy', 'sad', 'fear', 'surprise']
PHASE_3_EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']

print("Memory-optimized configuration loaded")
print(f"Effective batch size: {BATCH_SIZE * ACCUM_STEPS}")
print(f"Mixed precision enabled: {USE_AMP}")

class SqueezeNextBlock(nn.Module):
    """
    SqueezeNext block following Uddin (2025) specification
    """
    def __init__(self, in_channels, out_channels, stride=1, expansion=4):
        super(SqueezeNextBlock, self).__init__()

        # Expansion phase
        expanded_channels = in_channels * expansion

        # 1x1 convolution (expansion)
        self.conv1 = nn.Conv2d(in_channels, expanded_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(expanded_channels)

        # 3x3 depthwise convolution
        self.conv2 = nn.Conv2d(expanded_channels, expanded_channels, kernel_size=3,
                              stride=stride, padding=1, groups=expanded_channels, bias=False)
        self.bn2 = nn.BatchNorm2d(expanded_channels)

        # 1x1 convolution (projection)
        self.conv3 = nn.Conv2d(expanded_channels, out_channels, kernel_size=1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_channels)

        self.relu = nn.ReLU6(inplace=True)

        # Skip connection
        self.use_skip = stride == 1 and in_channels == out_channels

    def forward(self, x):
        identity = x

        # Expansion
        out = self.relu(self.bn1(self.conv1(x)))

        # Depthwise
        out = self.relu(self.bn2(self.conv2(out)))

        # Projection
        out = self.bn3(self.conv3(out))

        if self.use_skip:
            out += identity

        return self.relu(out)

class HybridChannelAttention(nn.Module):
    """
    Hybrid Channel Attention (HCA) from Uddin (2025)
    Combines Group Normalization with ECA attention
    """
    def __init__(self, channels, reduction=16, groups=8):
        super(HybridChannelAttention, self).__init__()

        # Group Normalization
        self.group_norm = nn.GroupNorm(groups, channels)

        # ECA attention
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # Efficient Channel Attention
        self.conv = nn.Conv1d(1, 1, kernel_size=3, padding=1, bias=False)
        self.sigmoid = nn.Sigmoid()

        # Channel reduction for efficiency
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels, bias=False)
        )

    def forward(self, x):
        # Apply group normalization
        x_norm = self.group_norm(x)

        # Global average pooling
        b, c, _, _ = x.size()
        avg_out = self.avg_pool(x_norm).view(b, c)
        max_out = self.max_pool(x_norm).view(b, c)

        # Channel attention
        avg_att = self.fc(avg_out)
        max_att = self.fc(max_out)

        # Combine and apply sigmoid
        channel_att = self.sigmoid(avg_att + max_att).view(b, c, 1, 1)

        return x * channel_att

class CoordinateSpaceAttention(nn.Module):
    """
    Coordinate Space Attention (CSA) from Uddin (2025)
    """
    def __init__(self, channels):
        super(CoordinateSpaceAttention, self).__init__()

        # Coordinate attention components
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))

        # Shared convolution
        self.conv1 = nn.Conv2d(channels, channels // 8, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(channels // 8)
        self.relu = nn.ReLU(inplace=True)

        # Separate convolutions for height and width
        self.conv_h = nn.Conv2d(channels // 8, channels, kernel_size=1, stride=1, padding=0)
        self.conv_w = nn.Conv2d(channels // 8, channels, kernel_size=1, stride=1, padding=0)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        n, c, h, w = x.size()

        # Coordinate pooling
        x_h = self.pool_h(x)  # (n, c, h, 1)
        x_w = self.pool_w(x).permute(0, 1, 3, 2)  # (n, c, w, 1)

        # Concatenate along height dimension
        y = torch.cat([x_h, x_w], dim=2)  # (n, c, h+w, 1)

        # Shared convolution
        y = self.relu(self.bn1(self.conv1(y)))

        # Split back
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)  # (n, c, 1, w)

        # Generate attention maps
        a_h = self.sigmoid(self.conv_h(x_h))  # (n, c, h, 1)
        a_w = self.sigmoid(self.conv_w(x_w))  # (n, c, 1, w)

        # Apply attention
        out = x * a_h * a_w

        return out

class DualAttentionConvolution(nn.Module):
    """
    Dual Attention Convolution (DAC) block from Uddin (2025)
    Combines HCA and CSA
    """
    def __init__(self, channels, reduction=16):
        super(DualAttentionConvolution, self).__init__()

        self.hca = HybridChannelAttention(channels, reduction)
        self.csa = CoordinateSpaceAttention(channels)

    def forward(self, x):
        # Apply channel attention first
        x = self.hca(x)

        # Apply spatial attention
        x = self.csa(x)

        return x

class DALDLSpatialExtractor(nn.Module):
    """
    DALDL Spatial Feature Extractor following Uddin (2025)
    Target: 0.75M parameters, 3.2MB model size
    """
    def __init__(self):
        super(DALDLSpatialExtractor, self).__init__()

        # Initial convolution
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.relu = nn.ReLU6(inplace=True)

        # SqueezeNext layers following Uddin's architecture
        self.layer1 = nn.Sequential(
            SqueezeNextBlock(32, 16, stride=1),
            SqueezeNextBlock(16, 24, stride=2),
        )

        self.layer2 = nn.Sequential(
            SqueezeNextBlock(24, 24, stride=1),
            SqueezeNextBlock(24, 32, stride=2),
        )

        self.layer3 = nn.Sequential(
            SqueezeNextBlock(32, 32, stride=1),
            SqueezeNextBlock(32, 64, stride=2),
            SqueezeNextBlock(64, 64, stride=1),
        )

        self.layer4 = nn.Sequential(
            SqueezeNextBlock(64, 96, stride=2),
            SqueezeNextBlock(96, 96, stride=1),
            SqueezeNextBlock(96, 160, stride=1),
        )

        # DAC block after layer4
        self.dac = DualAttentionConvolution(160, reduction=16)

        # Final convolution
        self.conv_final = nn.Conv2d(160, 320, kernel_size=1, bias=False)
        self.bn_final = nn.BatchNorm2d(320)

        # Global average pooling
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.2)

        self.feature_dim = 320

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        # Apply DAC block
        x = self.dac(x)

        # Final convolution
        x = self.relu(self.bn_final(self.conv_final(x)))

        # Global pooling
        x = self.avgpool(x)
        x = self.dropout(x)
        x = x.view(x.size(0), -1)

        return x

class TemporalAggregationModule(nn.Module):
    """
    Lightweight temporal aggregation for video sequences
    """
    def __init__(self, feature_dim=320, num_classes=6):
        super(TemporalAggregationModule, self).__init__()

        # Temporal convolution layers
        self.temporal_conv1 = nn.Conv1d(feature_dim, 128, kernel_size=3, padding=1)
        self.temporal_bn1 = nn.BatchNorm1d(128)

        self.temporal_conv2 = nn.Conv1d(128, 64, kernel_size=3, padding=1)
        self.temporal_bn2 = nn.BatchNorm1d(64)

        self.relu = nn.ReLU(inplace=True)
        self.dropout = nn.Dropout(0.3)

        # Temporal attention
        self.temporal_attention = nn.MultiheadAttention(
            embed_dim=64, num_heads=4, batch_first=True, dropout=0.1
        )

        # Global pooling
        self.global_pool = nn.AdaptiveAvgPool1d(1)

        # Classifier
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        # x shape: (batch_size, seq_len, feature_dim)

        # Transpose for 1D convolution
        x = x.transpose(1, 2)  # (batch_size, feature_dim, seq_len)

        # Temporal convolutions
        x = self.relu(self.temporal_bn1(self.temporal_conv1(x)))
        x = self.dropout(x)

        x = self.relu(self.temporal_bn2(self.temporal_conv2(x)))
        x = self.dropout(x)

        # Transpose back for attention
        x = x.transpose(1, 2)  # (batch_size, seq_len, 64)

        # Temporal attention
        attn_out, _ = self.temporal_attention(x, x, x)
        x = x + attn_out  # Residual connection

        # Transpose for pooling
        x = x.transpose(1, 2)  # (batch_size, 64, seq_len)

        # Global pooling
        x = self.global_pool(x)
        x = x.squeeze(-1)

        # Classification
        x = self.classifier(x)

        return x

class VideoDALDL(nn.Module):
    """
    Complete Video-DALDL model extending Uddin (2025)
    """
    def __init__(self, num_classes=6, freeze_spatial=True):
        super(VideoDALDL, self).__init__()

        # DALDL spatial extractor
        self.spatial_extractor = DALDLSpatialExtractor()

        # Optionally freeze spatial extractor
        if freeze_spatial:
            for param in self.spatial_extractor.parameters():
                param.requires_grad = False

        # Temporal aggregation module
        self.temporal_aggregator = TemporalAggregationModule(
            feature_dim=self.spatial_extractor.feature_dim,
            num_classes=num_classes
        )

    def forward(self, x):
        # x shape: (batch_size, seq_len, channels, height, width)
        batch_size, seq_len = x.size(0), x.size(1)

        # Process all frames
        x = x.view(batch_size * seq_len, *x.shape[2:])
        spatial_features = self.spatial_extractor(x)

        # Reshape for temporal processing
        spatial_features = spatial_features.view(batch_size, seq_len, -1)

        # Temporal aggregation
        output = self.temporal_aggregator(spatial_features)

        return output

print("DALDL architecture loaded following Uddin (2025)")

class KMUFEDDataset(Dataset):
    """
    Enhanced dataset following Uddin (2025) preprocessing
    """
    def __init__(self, sequences, labels, transform=None, augment_factor=1):
        self.original_sequences = sequences
        self.original_labels = labels
        self.transform = transform

        # Create augmented dataset
        self.sequences = []
        self.labels = []

        for _ in range(augment_factor):
            self.sequences.extend(sequences)
            self.labels.extend(labels)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        label = self.labels[idx]

        # Apply transforms to each frame
        if self.transform:
            sequence = [self.transform(frame) for frame in sequence]

        # Stack frames into tensor
        sequence = torch.stack(sequence)

        return sequence, label

def preprocess_images_uddin(image_path, target_size=(224, 224)):
    """
    Preprocessing following Uddin (2025) methodology
    """
    try:
        # Read image
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            raise ValueError(f"Could not load image: {image_path}")

        # Resize
        img = cv2.resize(img, target_size)

        # Histogram equalization (as mentioned in Uddin's paper)
        img = cv2.equalizeHist(img)

        # Gaussian blur for noise reduction
        img = cv2.GaussianBlur(img, (3, 3), 0)

        # Convert to PIL for transforms
        img = Image.fromarray(img)

        return img

    except Exception as e:
        print(f"Error processing image {image_path}: {e}")
        return None

def create_video_sequences_uddin(data_dir, sequence_length=4, overlap=0.5):
    """
    Create video sequences following Uddin's data methodology
    """
    sequences = []
    labels = []

    emotion_map = {
        'angry': 0, 'disgust': 1, 'fear': 2,
        'happy': 3, 'sad': 4, 'surprise': 5
    }

    try:
        print(f"Processing dataset from: {data_dir}")

        for emotion_folder in os.listdir(data_dir):
            emotion_path = os.path.join(data_dir, emotion_folder)

            if not os.path.isdir(emotion_path):
                continue

            emotion_key = emotion_folder.lower()
            if emotion_key not in emotion_map:
                continue

            print(f"Processing emotion: {emotion_folder}")

            image_files = sorted([f for f in os.listdir(emotion_path)
                                if f.lower().endswith(('.png', '.jpg', '.jpeg'))])

            # Group by subject
            subject_groups = {}
            for img_file in image_files:
                subject_id = img_file.split('_')[0] if '_' in img_file else 'default'
                if subject_id not in subject_groups:
                    subject_groups[subject_id] = []
                subject_groups[subject_id].append(img_file)

            # Create sequences with overlap
            for subject_images in subject_groups.values():
                subject_images.sort()
                step_size = max(1, int(sequence_length * (1 - overlap)))

                for i in range(0, len(subject_images) - sequence_length + 1, step_size):
                    sequence_frames = []

                    for j in range(i, i + sequence_length):
                        img_path = os.path.join(emotion_path, subject_images[j])
                        processed_img = preprocess_images_uddin(img_path)

                        if processed_img is not None:
                            sequence_frames.append(processed_img)

                    if len(sequence_frames) == sequence_length:
                        sequences.append(sequence_frames)
                        labels.append(emotion_map[emotion_key])

        print(f"Total sequences created: {len(sequences)}")
        return sequences, labels

    except Exception as e:
        print(f"Error creating sequences: {e}")
        return [], []

def get_uddin_transforms():
    """
    Data transforms following Uddin (2025) methodology
    """
    # Training transforms with augmentation
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])

    # Validation transforms
    val_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5])
    ])

    return train_transform, val_transform

print("Dataset processing loaded following Uddin (2025)")

def progressive_training_memory_optimized(data_dir, phase_emotions, phase_name, num_epochs=100,
                                        prev_model_path=None):
    """
    Memory-optimized progressive training with gradient accumulation and mixed precision
    """
    # Clear GPU memory before starting new phase
    torch.cuda.empty_cache()
    gc.collect()

    print(f"\n{'='*60}")
    print(f"PHASE: {phase_name}")
    print(f"EMOTIONS: {phase_emotions}")
    print(f"GPU Memory before phase: {torch.cuda.memory_allocated()/1024**3:.2f} GB")
    print(f"{'='*60}")

    # Create emotion mapping
    emotion_map = {emotion: idx for idx, emotion in enumerate(phase_emotions)}
    num_classes = len(phase_emotions)

    # Get all sequences
    all_sequences, all_labels = create_video_sequences_uddin(data_dir, SEQUENCE_LENGTH)

    # Filter for current phase
    phase_sequences = []
    phase_labels = []
    emotion_names = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise']

    for seq, label in zip(all_sequences, all_labels):
        emotion_name = emotion_names[label]
        if emotion_name in phase_emotions:
            phase_sequences.append(seq)
            phase_labels.append(emotion_map[emotion_name])

    print(f"Phase dataset size: {len(phase_sequences)} sequences")

    # Get transforms
    train_transform, val_transform = get_uddin_transforms()

    # Split data
    train_sequences, val_sequences, train_labels, val_labels = train_test_split(
        phase_sequences, phase_labels, test_size=0.2, random_state=42,
        stratify=phase_labels if len(set(phase_labels)) > 1 else None
    )

    # Create datasets with reduced augmentation to save memory
    train_dataset = KMUFEDDataset(train_sequences, train_labels, train_transform,
                                 augment_factor=4)  # Reduced from 8 to 4
    val_dataset = KMUFEDDataset(val_sequences, val_labels, val_transform)

    # Memory-efficient data loaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                             num_workers=2, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                           num_workers=2, pin_memory=True)

    print(f"Training samples: {len(train_dataset)} (with 4x augmentation)")
    print(f"Validation samples: {len(val_dataset)}")
    print(f"Physical batch size: {BATCH_SIZE}, Effective batch size: {BATCH_SIZE * ACCUM_STEPS}")

    # Create model with frozen spatial extractor for Phase 1
    freeze_spatial = (phase_name == "Phase 1: Binary")
    model = VideoDALDL(num_classes=num_classes, freeze_spatial=freeze_spatial)

    # Transfer learning from previous phase
    if prev_model_path and os.path.exists(prev_model_path):
        try:
            prev_state = torch.load(prev_model_path, map_location='cpu')  # Load to CPU first

            # Load spatial extractor weights
            spatial_dict = {k.replace('spatial_extractor.', ''): v
                          for k, v in prev_state.items()
                          if k.startswith('spatial_extractor.')}
            model.spatial_extractor.load_state_dict(spatial_dict, strict=False)

            # Load compatible temporal weights
            temporal_dict = {k.replace('temporal_aggregator.', ''): v
                           for k, v in prev_state.items()
                           if k.startswith('temporal_aggregator.') and
                           not k.endswith('classifier.weight') and
                           not k.endswith('classifier.bias')}
            model.temporal_aggregator.load_state_dict(temporal_dict, strict=False)

            print(f"Loaded weights from previous phase: {prev_model_path}")
            del prev_state  # Free memory

        except Exception as e:
            print(f"Could not load previous weights: {e}")

    model = model.to(device)

    # Print model info
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"GPU Memory after model load: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

    # Training setup with mixed precision
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    scaler = GradScaler(enabled=USE_AMP)

    best_val_acc = 0.0
    patience_counter = 0

    for epoch in range(num_epochs):
        # Training phase with gradient accumulation and mixed precision
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        # Zero gradients at start of epoch
        optimizer.zero_grad(set_to_none=True)

        for batch_idx, (sequences, labels) in enumerate(train_loader):
            sequences = sequences.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # Mixed precision forward pass
            with autocast(enabled=USE_AMP):
                outputs = model(sequences)
                loss = criterion(outputs, labels) / ACCUM_STEPS  # Scale loss for accumulation

            # Mixed precision backward pass
            scaler.scale(loss).backward()

            # Accumulate gradients
            if (batch_idx + 1) % ACCUM_STEPS == 0:
                # Gradient clipping
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                # Optimizer step
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)

            # Statistics
            train_loss += loss.item() * ACCUM_STEPS  # Unscale for logging
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()

            # Memory cleanup
            del sequences, labels, outputs, loss

            if batch_idx % 20 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], '
                      f'GPU Mem: {torch.cuda.memory_allocated()/1024**3:.2f} GB')

        # Handle remaining gradients if batch doesn't divide evenly
        if len(train_loader) % ACCUM_STEPS != 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for sequences, labels in val_loader:
                sequences = sequences.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True)

                with autocast(enabled=USE_AMP):
                    outputs = model(sequences)
                    loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

                # Memory cleanup
                del sequences, labels, outputs, loss

        train_acc = 100 * train_correct / train_total
        val_acc = 100 * val_correct / val_total

        print(f'Epoch [{epoch+1}/{num_epochs}] - Train Acc: {train_acc:.2f}%, '
              f'Val Acc: {val_acc:.2f}%, LR: {scheduler.get_last_lr()[0]:.6f}')
        print(f'GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB')

        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            model_path = f'/content/best_model_{phase_name.lower().replace(" ", "_").replace(":", "")}.pth'
            torch.save(model.state_dict(), model_path)
            patience_counter = 0
            print(f'  New best model saved! Val Acc: {val_acc:.2f}%')
        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= PATIENCE:
            print(f'Early stopping at epoch {epoch+1}')
            break

        scheduler.step()

        # Clear cache periodically
        if epoch % 5 == 0:
            torch.cuda.empty_cache()

    print(f'Phase completed. Best validation accuracy: {best_val_acc:.2f}%')

    # Final cleanup
    torch.cuda.empty_cache()
    gc.collect()

    # Return model path for next phase
    model_path = f'/content/best_model_{phase_name.lower().replace(" ", "_").replace(":", "")}.pth'
    return model, best_val_acc, model_path

print("Memory-optimized progressive training function loaded")

import torch
torch.cuda.empty_cache()  # Clear cached memory

def main_uddin_extension_memory_optimized():
    """
    Memory-optimized main function with proper cleanup between phases
    """
    print("Memory-Optimized Video-DALDL Extension of Uddin (2025)")
    print("="*60)

    # Initial memory cleanup
    torch.cuda.empty_cache()
    gc.collect()

    results = {}
    model_paths = {}

    try:
        # Phase 1: Binary classification (Happy vs Sad)
        print("\nðŸš€ Phase 1: Binary Classification (Happy vs Sad)")
        print(f"Initial GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

        model_phase1, acc_phase1, path_phase1 = progressive_training_memory_optimized(
            DATA_DIR, PHASE_1_EMOTIONS, "Phase 1: Binary", NUM_EPOCHS
        )
        results['Phase 1'] = acc_phase1
        model_paths['Phase 1'] = path_phase1

        # Cleanup after Phase 1
        del model_phase1
        torch.cuda.empty_cache()
        gc.collect()
        print(f"GPU Memory after Phase 1 cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

        # Phase 2: Four emotions with transfer learning
        print("\nðŸš€ Phase 2: Four Emotions with Transfer Learning")
        model_phase2, acc_phase2, path_phase2 = progressive_training_memory_optimized(
            DATA_DIR, PHASE_2_EMOTIONS, "Phase 2: Four Emotions", NUM_EPOCHS,
            prev_model_path=path_phase1
        )
        results['Phase 2'] = acc_phase2
        model_paths['Phase 2'] = path_phase2

        # Cleanup after Phase 2
        del model_phase2
        torch.cuda.empty_cache()
        gc.collect()
        print(f"GPU Memory after Phase 2 cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

        # Phase 3: All six emotions with transfer learning
        print("\nðŸš€ Phase 3: All Six Emotions with Transfer Learning")
        model_phase3, acc_phase3, path_phase3 = progressive_training_memory_optimized(
            DATA_DIR, PHASE_3_EMOTIONS, "Phase 3: All Emotions", NUM_EPOCHS,
            prev_model_path=path_phase2
        )
        results['Phase 3'] = acc_phase3
        model_paths['Phase 3'] = path_phase3

        # Final evaluation
        print("\n" + "="*70)
        print("FINAL RESULTS - Memory-Optimized Video-DALDL Extension")
        print("="*70)

        for phase, accuracy in results.items():
            improvement = "âœ…" if accuracy > 50 else "âš ï¸" if accuracy > 30 else "âŒ"
            print(f"{phase}: {accuracy:.2f}% {improvement}")

        # Load final model for evaluation
        final_model = VideoDALDL(num_classes=6, freeze_spatial=False)
        final_model.load_state_dict(torch.load(path_phase3, map_location=device))
        final_model = final_model.to(device)

        # Model statistics
        total_params = sum(p.numel() for p in final_model.parameters())
        model_size = sum(p.numel() * p.element_size() for p in final_model.parameters()) / (1024 * 1024)

        print(f"\nFinal Model Statistics:")
        print(f"Total Parameters: {total_params:,}")
        print(f"Model Size: {model_size:.2f} MB")
        print(f"Final GPU Memory Usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

        # Inference speed test with memory monitoring
        print(f"\nInference Speed Test:")
        dummy_input = torch.randn(1, 4, 1, 224, 224).to(device)

        # Warmup
        final_model.eval()
        with torch.no_grad():
            for _ in range(10):
                with autocast(enabled=USE_AMP):
                    _ = final_model(dummy_input)

        # Measure
        times = []
        with torch.no_grad():
            for _ in range(100):
                start_time = time.time()
                with autocast(enabled=USE_AMP):
                    _ = final_model(dummy_input)
                torch.cuda.synchronize()  # Ensure GPU completion
                end_time = time.time()
                times.append((end_time - start_time) * 1000)

        avg_inference_time = np.mean(times)
        std_inference_time = np.std(times)

        print(f"Average Inference Time: {avg_inference_time:.2f} Â± {std_inference_time:.2f} ms")

        # Target assessment
        print(f"\n" + "="*70)
        print("TARGET ASSESSMENT vs Uddin (2025) Baseline")
        print("="*70)
        print(f"Final Accuracy: {acc_phase3:.2f}% (Target: >88.9%)")
        print(f"Model Size: {model_size:.2f} MB (Target: <3.3 MB)")
        print(f"Inference Time: {avg_inference_time:.2f} ms (Target: <5 ms)")

        # Achievement status
        accuracy_achieved = acc_phase3 > 88.9
        size_achieved = model_size < 3.3
        speed_achieved = avg_inference_time < 5.0

        print(f"\nAchievement Status:")
        print(f"{'âœ…' if accuracy_achieved else 'âŒ'} Accuracy Target: {'ACHIEVED' if accuracy_achieved else 'NOT ACHIEVED'}")
        print(f"{'âœ…' if size_achieved else 'âŒ'} Model Size Target: {'ACHIEVED' if size_achieved else 'NOT ACHIEVED'}")
        print(f"{'âœ…' if speed_achieved else 'âŒ'} Inference Speed Target: {'ACHIEVED' if speed_achieved else 'NOT ACHIEVED'}")

        if accuracy_achieved and size_achieved and speed_achieved:
            print(f"\nðŸŽ‰ ALL TARGETS ACHIEVED! Video-DALDL extension successful!")
        else:
            print(f"\nâš ï¸ Some targets need improvement. Continue optimization.")

        # Save final model
        torch.save(final_model.state_dict(), '/content/final_video_daldl_uddin_optimized.pth')
        print(f"\nFinal model saved to: /content/final_video_daldl_uddin_optimized.pth")

        # Final cleanup
        del final_model, dummy_input
        torch.cuda.empty_cache()
        gc.collect()

        return results

    except Exception as e:
        print(f"Error in main execution: {e}")
        import traceback
        traceback.print_exc()

        # Emergency cleanup
        torch.cuda.empty_cache()
        gc.collect()

        return {}

# Execute the memory-optimized main function
if __name__ == "__main__":
    training_results = main_uddin_extension_memory_optimized()

def analyze_model_performance(model, results):
    """
    Analyze and visualize model performance
    """
    print("Model Performance Analysis")
    print("="*40)

    # Architecture analysis
    total_params = sum(p.numel() for p in model.parameters())
    spatial_params = sum(p.numel() for p in model.spatial_extractor.parameters())
    temporal_params = sum(p.numel() for p in model.temporal_aggregator.parameters())

    print(f"Architecture Breakdown:")
    print(f"  Spatial Extractor: {spatial_params:,} parameters ({spatial_params/total_params*100:.1f}%)")
    print(f"  Temporal Aggregator: {temporal_params:,} parameters ({temporal_params/total_params*100:.1f}%)")
    print(f"  Total: {total_params:,} parameters")

    # Performance progression
    print(f"\nProgressive Training Results:")
    for phase, accuracy in results.items():
        print(f"  {phase}: {accuracy:.2f}%")

    # Visualization
    phases = list(results.keys())
    accuracies = list(results.values())

    plt.figure(figsize=(10, 6))
    plt.plot(phases, accuracies, 'bo-', linewidth=2, markersize=8)
    plt.axhline(y=88.9, color='r', linestyle='--', label='Uddin (2025) Baseline: 88.9%')
    plt.title('Progressive Training Performance')
    plt.xlabel('Training Phase')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # Model comparison
    print(f"\nComparison with Uddin (2025):")
    print(f"  Original DALDL: 88.9% accuracy, 0.75M params, 3.2MB, 3.9ms")
    print(f"  Video-DALDL: {accuracies[-1]:.2f}% accuracy, {total_params/1000000:.2f}M params")

    return total_params

# Run analysis if model exists
if 'final_model' in locals() and final_model is not None:
    analyze_model_performance(final_model, training_results)
else:
    print("Run the main training function first to analyze results")